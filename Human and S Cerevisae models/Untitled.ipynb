{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e70bc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laura\\anaconda3\\envs\\warsztaty_badawcze\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# note that this custom dataset is not prepared on the top of geometric Dataset(pytorch's inbuilt)\n",
    "import os\n",
    "import torch\n",
    "import glob\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50ef00a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In PyG 1.*\n",
    "# data = torch.load(\"./PPI_GNN-main/Human_features/processed/10GS.pt\")\n",
    "# torch.save(data.to_dict(), \"./PPI_GNN-main/Human_features/processed_new/10GS.pt\")\n",
    "\n",
    "# # # In PyG 2.*\n",
    "# # data_dict = torch.load('data_dict.pt')\n",
    "# # data = Data.from_dict(data_dict)\n",
    "# # torch.save(data, 'data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94afb6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in listdir(\"./PPI_GNN-main/Human_features/processed/\"):\n",
    "#     dir = os.path.join(\"./PPI_GNN-main/Human_features/processed/\", x)\n",
    "#     dir2 = os.path.join(\"./PPI_GNN-main/Human_features/processed_new/\", x)\n",
    "#     print(x)\n",
    "\n",
    "#     # data = torch.load(dir)\n",
    "#     # torch.save(data.to_dict(), dir2)\n",
    "\n",
    "#     data_dict = torch.load(dir)\n",
    "#     data = pd.DataFrame.from_dict(data_dict)\n",
    "#     torch.save(data, dir2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870a92b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d36e4b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE: (22217, 7)\n",
      "[['NP_663777' 'Q13114' '1FLK' ... 'P26842' '5TL5' '1']\n",
      " ['NP_003630' 'Q9Y6K9' '2JVX' ... 'Q14790' '1F9E' '1']\n",
      " ['NP_003001' 'P45985' '3ALN' ... 'Q12852' '5CEN' '1']\n",
      " ...\n",
      " ['NP_000779' 'P27707' '1P5Z' ... 'P00338' '1I10' '0']\n",
      " ['NP_008828' 'P31274' '2LP0' ... 'Q9H257' '6E25' '0']\n",
      " ['NP_001740' 'P04632' '1KFU' ... 'Q15052' '1UJY' '1']]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected 'Iterator' as the return annotation for `__iter__` of SMILESParser, but found typing.Any",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(npy_ar)\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset \u001b[39mas\u001b[39;00m Dataset_n\n\u001b[1;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch_geometric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader \u001b[39mas\u001b[39;00m DataLoader_n\n",
      "File \u001b[1;32mc:\\Users\\laura\\anaconda3\\envs\\warsztaty_badawcze\\lib\\site-packages\\torch_geometric\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch_geometric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch_geometric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch_geometric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msampler\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch_geometric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloader\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\laura\\anaconda3\\envs\\warsztaty_badawcze\\lib\\site-packages\\torch_geometric\\data\\__init__.py:48\u001b[0m\n\u001b[0;32m     44\u001b[0m lightning \u001b[39m=\u001b[39m LazyLoader(\u001b[39m'\u001b[39m\u001b[39mlightning\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mglobals\u001b[39m(),\n\u001b[0;32m     45\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mtorch_geometric.data.lightning\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch_geometric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdeprecation\u001b[39;00m \u001b[39mimport\u001b[39;00m deprecated\n\u001b[1;32m---> 48\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch_geometric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloader\u001b[39;00m \u001b[39mimport\u001b[39;00m NeighborSampler\n\u001b[0;32m     49\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch_geometric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloader\u001b[39;00m \u001b[39mimport\u001b[39;00m ClusterData\n\u001b[0;32m     50\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch_geometric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloader\u001b[39;00m \u001b[39mimport\u001b[39;00m ClusterLoader\n",
      "File \u001b[1;32mc:\\Users\\laura\\anaconda3\\envs\\warsztaty_badawcze\\lib\\site-packages\\torch_geometric\\loader\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch_geometric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdeprecation\u001b[39;00m \u001b[39mimport\u001b[39;00m deprecated\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdataloader\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mnode_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m NodeLoader\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlink_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m LinkLoader\n",
      "File \u001b[1;32mc:\\Users\\laura\\anaconda3\\envs\\warsztaty_badawcze\\lib\\site-packages\\torch_geometric\\loader\\dataloader.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch_geometric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Batch, Dataset\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch_geometric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseData\n\u001b[1;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch_geometric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatapipes\u001b[39;00m \u001b[39mimport\u001b[39;00m DatasetAdapter\n\u001b[0;32m     12\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCollater\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, follow_batch, exclude_keys):\n",
      "File \u001b[1;32mc:\\Users\\laura\\anaconda3\\envs\\warsztaty_badawcze\\lib\\site-packages\\torch_geometric\\data\\datapipes.py:36\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     22\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m     23\u001b[0m         dp: IterDataPipe,\n\u001b[0;32m     24\u001b[0m         batch_size: \u001b[39mint\u001b[39m,\n\u001b[0;32m     25\u001b[0m         drop_last: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     26\u001b[0m     ):\n\u001b[0;32m     27\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m     28\u001b[0m             dp,\n\u001b[0;32m     29\u001b[0m             batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m     30\u001b[0m             drop_last\u001b[39m=\u001b[39mdrop_last,\n\u001b[0;32m     31\u001b[0m             wrapper_class\u001b[39m=\u001b[39mBatch\u001b[39m.\u001b[39mfrom_data_list,\n\u001b[0;32m     32\u001b[0m         )\n\u001b[0;32m     35\u001b[0m \u001b[39m@functional_datapipe\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mparse_smiles\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mSMILESParser\u001b[39;00m(IterDataPipe):\n\u001b[0;32m     37\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     38\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m     39\u001b[0m         dp: IterDataPipe,\n\u001b[0;32m     40\u001b[0m         smiles_key: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msmiles\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     41\u001b[0m         target_key: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     42\u001b[0m     ):\n\u001b[0;32m     43\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\laura\\anaconda3\\envs\\warsztaty_badawcze\\lib\\site-packages\\torch\\utils\\data\\_typing.py:273\u001b[0m, in \u001b[0;36m_DataPipeMeta.__new__\u001b[1;34m(cls, name, bases, namespace, **kwargs)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[39mfor\u001b[39;00m base \u001b[39min\u001b[39;00m bases:\n\u001b[0;32m    272\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(base, _DataPipeMeta):\n\u001b[1;32m--> 273\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__new__\u001b[39m(\u001b[39mcls\u001b[39m, name, bases, namespace, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m    275\u001b[0m namespace\u001b[39m.\u001b[39mupdate({\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m: _DEFAULT_TYPE,\n\u001b[0;32m    276\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39m__init_subclass__\u001b[39m\u001b[39m'\u001b[39m: _dp_init_subclass})\n\u001b[0;32m    277\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__new__\u001b[39m(\u001b[39mcls\u001b[39m, name, bases, namespace, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\laura\\anaconda3\\envs\\warsztaty_badawcze\\lib\\abc.py:106\u001b[0m, in \u001b[0;36mABCMeta.__new__\u001b[1;34m(mcls, name, bases, namespace, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__new__\u001b[39m(mcls, name, bases, namespace, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 106\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__new__\u001b[39m(mcls, name, bases, namespace, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    107\u001b[0m     _abc_init(\u001b[39mcls\u001b[39m)\n\u001b[0;32m    108\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\laura\\anaconda3\\envs\\warsztaty_badawcze\\lib\\site-packages\\torch\\utils\\data\\_typing.py:373\u001b[0m, in \u001b[0;36m_dp_init_subclass\u001b[1;34m(sub_cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    369\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mhasattr\u001b[39m(return_hint, \u001b[39m'\u001b[39m\u001b[39m__origin__\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m\n\u001b[0;32m    371\u001b[0m         (return_hint\u001b[39m.\u001b[39m__origin__ \u001b[39m==\u001b[39m Iterator \u001b[39mor\u001b[39;00m\n\u001b[0;32m    372\u001b[0m          return_hint\u001b[39m.\u001b[39m__origin__ \u001b[39m==\u001b[39m collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mIterator)):\n\u001b[1;32m--> 373\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mExpected \u001b[39m\u001b[39m'\u001b[39m\u001b[39mIterator\u001b[39m\u001b[39m'\u001b[39m\u001b[39m as the return annotation for `__iter__` of \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    374\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39m, but found \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(sub_cls\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, _type_repr(hints[\u001b[39m'\u001b[39m\u001b[39mreturn\u001b[39m\u001b[39m'\u001b[39m])))\n\u001b[0;32m    375\u001b[0m data_type \u001b[39m=\u001b[39m return_hint\u001b[39m.\u001b[39m__args__[\u001b[39m0\u001b[39m]\n\u001b[0;32m    376\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m issubtype(data_type, sub_cls\u001b[39m.\u001b[39mtype\u001b[39m.\u001b[39mparam):\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected 'Iterator' as the return annotation for `__iter__` of SMILESParser, but found typing.Any"
     ]
    }
   ],
   "source": [
    "\n",
    "processed_dir = \"./PPI_GNN-main/Human_features/processed/\"\n",
    "npy_file = \"./PPI_GNN-main/Human_features/npy_file_new(human_dataset).npy\"\n",
    "npy_ar = np.load(npy_file)\n",
    "print(\"SHAPE:\", npy_ar.shape)\n",
    "print(npy_ar)\n",
    "\n",
    "from torch.utils.data import Dataset as Dataset_n\n",
    "from torch_geometric.data import DataLoader as DataLoader_n\n",
    "\n",
    "class LabelledDataset(Dataset_n):\n",
    "    def __init__(self, npy_file, processed_dir):\n",
    "      self.npy_ar = np.load(npy_file)\n",
    "      self.processed_dir = processed_dir\n",
    "      self.protein_1 = self.npy_ar[:,2]\n",
    "      self.protein_2 = self.npy_ar[:,5]\n",
    "      self.label = self.npy_ar[:,6].astype(float)\n",
    "      self.n_samples = self.npy_ar.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "      return(self.n_samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "      prot_1 = os.path.join(self.processed_dir, self.protein_1[index]+\".pt\")\n",
    "      prot_2 = os.path.join(self.processed_dir, self.protein_2[index]+\".pt\")\n",
    "      prot_1 = torch.load(glob.glob(prot_1)[0])\n",
    "      prot_2 = torch.load(glob.glob(prot_2)[0])\n",
    "      return prot_1, prot_2, torch.tensor(self.label[index])\n",
    "\n",
    "\n",
    "\n",
    "dataset = LabelledDataset(npy_file = npy_file ,processed_dir=processed_dir)\n",
    "\n",
    "final_pairs =  np.load(npy_file)\n",
    "size = final_pairs.shape[0]\n",
    "print(\"\\nSize:\", size)\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "trainset, testset = torch.utils.data.random_split(dataset, [math.floor(0.8 * size), size - math.floor(0.8 * size) ])\n",
    "trainloader = DataLoader_n(dataset= trainset, batch_size= 4, num_workers = 0)\n",
    "testloader = DataLoader_n(dataset= testset, batch_size= 4, num_workers = 0)\n",
    "\n",
    "print(\"\\nLength:\")\n",
    "print(len(trainloader))\n",
    "print(len(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "61180b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = len(dataset)\n",
    "n_iterations = math.ceil(total_samples/5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5a2f5972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_samples: 22217 n_iterations: 4444\n"
     ]
    }
   ],
   "source": [
    "print(\"total_samples:\", total_samples, \"n_iterations:\", n_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCNN Loaded\n",
      "GCNN(\n",
      "  (pro1_conv1): GCNConv(1024, 1024)\n",
      "  (pro1_fc1): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  (pro2_conv1): GCNConv(1024, 1024)\n",
      "  (pro2_fc1): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  (relu): LeakyReLU(negative_slope=0.01)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (out): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "AttGNN Loaded\n",
      "AttGNN(\n",
      "  (pro1_conv1): GATConv(1024, 128, heads=1)\n",
      "  (pro1_fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (pro2_conv1): GATConv(1024, 128, heads=1)\n",
      "  (pro2_fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (relu): LeakyReLU(negative_slope=0.01)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (out): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Building model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_max_pool as gmp, global_add_pool as gap,global_mean_pool as gep,global_sort_pool\n",
    "from torch_geometric.utils import dropout_adj\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "\n",
    "\n",
    "class GCNN(nn.Module):\n",
    "    def __init__(self, n_output=1, num_features_pro= 1024, output_dim=128, dropout=0.2):\n",
    "        super(GCNN, self).__init__()\n",
    "\n",
    "        print('GCNN Loaded')\n",
    "\n",
    "        # for protein 1\n",
    "        self.n_output = n_output\n",
    "        self.pro1_conv1 = GCNConv(num_features_pro, num_features_pro)\n",
    "        self.pro1_fc1 = nn.Linear(num_features_pro, output_dim)\n",
    "\n",
    "        # for protein 2\n",
    "        self.pro2_conv1 = GCNConv(num_features_pro, num_features_pro)\n",
    "        self.pro2_fc1 = nn.Linear(num_features_pro, output_dim)\n",
    "\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # combined layers\n",
    "        self.fc1 = nn.Linear(2 * output_dim, 256)\n",
    "        self.fc2 = nn.Linear(256 ,64)\n",
    "        self.out = nn.Linear(64, self.n_output)\n",
    "\n",
    "    def forward(self, pro1_data, pro2_data):\n",
    "\n",
    "        #get graph input for protein 1 \n",
    "        pro1_x, pro1_edge_index, pro1_batch = pro1_data.x, pro1_data.edge_index, pro1_data.batch\n",
    "        # get graph input for protein 2\n",
    "        pro2_x, pro2_edge_index, pro2_batch = pro2_data.x, pro2_data.edge_index, pro2_data.batch\n",
    "\n",
    "        x = self.pro1_conv1(pro1_x, pro1_edge_index)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "\t# global pooling\n",
    "        x = gep(x, pro1_batch)   \n",
    "\n",
    "        # flatten\n",
    "        x = self.relu(self.pro1_fc1(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        xt = self.pro2_conv1(pro2_x, pro2_edge_index)\n",
    "        xt = self.relu(xt)\n",
    "\n",
    "\t# global pooling\n",
    "        xt = gep(xt, pro2_batch)  \n",
    "\n",
    "        # flatten\n",
    "        xt = self.relu(self.pro2_fc1(xt))\n",
    "        xt = self.dropout(xt)\n",
    "\n",
    "\t# Concatenation  \n",
    "        xc = torch.cat((x, xt), 1)\n",
    "\n",
    "        # add some dense layers\n",
    "        xc = self.fc1(xc)\n",
    "        xc = self.relu(xc)\n",
    "        xc = self.dropout(xc)\n",
    "        xc = self.fc2(xc)\n",
    "        xc = self.relu(xc)\n",
    "        xc = self.dropout(xc)\n",
    "        out = self.out(xc)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "        \n",
    "\n",
    "net = GCNN()\n",
    "print(net)\n",
    "\n",
    "\"\"\"# GAT\"\"\"\n",
    "\n",
    "class AttGNN(nn.Module):\n",
    "    def __init__(self, n_output=1, num_features_pro= 1024, output_dim=128, dropout=0.2, heads = 1 ):\n",
    "        super(AttGNN, self).__init__()\n",
    "\n",
    "        print('AttGNN Loaded')\n",
    "\n",
    "        self.hidden = 8\n",
    "        self.heads = 1\n",
    "        \n",
    "        # for protein 1\n",
    "        self.pro1_conv1 = GATConv(num_features_pro, self.hidden* 16, heads=self.heads, dropout=0.2)\n",
    "        self.pro1_fc1 = nn.Linear(128, output_dim)\n",
    "\n",
    "\n",
    "        # for protein 2\n",
    "        self.pro2_conv1 = GATConv(num_features_pro, self.hidden*16, heads=self.heads, dropout=0.2)\n",
    "        self.pro2_fc1 = nn.Linear(128, output_dim)\n",
    "\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # combined layers\n",
    "        self.fc1 = nn.Linear(2 * output_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.out = nn.Linear(64, n_output)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, pro1_data, pro2_data):\n",
    "\n",
    "        # get graph input for protein 1 \n",
    "        pro1_x, pro1_edge_index, pro1_batch = pro1_data.x, pro1_data.edge_index, pro1_data.batch\n",
    "        # get graph input for protein 2\n",
    "        pro2_x, pro2_edge_index, pro2_batch = pro2_data.x, pro2_data.edge_index, pro2_data.batch\n",
    "         \n",
    "        \n",
    "        x = self.pro1_conv1(pro1_x, pro1_edge_index)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "\t# global pooling\n",
    "        x = gep(x, pro1_batch)  \n",
    "       \n",
    "        # flatten\n",
    "        x = self.relu(self.pro1_fc1(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "\n",
    "\n",
    "        xt = self.pro2_conv1(pro2_x, pro2_edge_index)\n",
    "        xt = self.relu(self.pro2_fc1(xt))\n",
    "\t\n",
    "\t# global pooling\n",
    "        xt = gep(xt, pro2_batch)  \n",
    "\n",
    "        # flatten\n",
    "        xt = self.relu(xt)\n",
    "        xt = self.dropout(xt)\n",
    "\n",
    "\t\n",
    "\t# Concatenation\n",
    "        xc = torch.cat((x, xt), 1)\n",
    "\n",
    "        # add some dense layers\n",
    "        xc = self.fc1(xc)\n",
    "        xc = self.relu(xc)\n",
    "        xc = self.dropout(xc)\n",
    "        xc = self.fc2(xc)\n",
    "        xc = self.relu(xc)\n",
    "        xc = self.dropout(xc)\n",
    "        out = self.out(xc)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "net_GAT = AttGNN()\n",
    "print(net_GAT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0572fccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCNN Loaded\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "\n",
      "\n",
      "Training on 4444 samples.....\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\laura\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\laura\\AppData\\Local\\Temp\\ipykernel_9020\\475155841.py\", line 62, in <module>\n",
      "    train(model, trainloader, optimizer, epoch+1)\n",
      "  File \"C:\\Users\\laura\\AppData\\Local\\Temp\\ipykernel_9020\\475155841.py\", line 14, in train\n",
      "    for count, (prot_1, prot_2, label) in enumerate(trainloader):\n",
      "  File \"c:\\Users\\laura\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 681, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"c:\\Users\\laura\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 721, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"c:\\Users\\laura\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"c:\\Users\\laura\\anaconda3\\lib\\site-packages\\torch_geometric\\loader\\dataloader.py\", line 35, in __call__\n",
      "    return [self(s) for s in zip(*batch)]\n",
      "  File \"c:\\Users\\laura\\anaconda3\\lib\\site-packages\\torch_geometric\\loader\\dataloader.py\", line 35, in <listcomp>\n",
      "    return [self(s) for s in zip(*batch)]\n",
      "  File \"c:\\Users\\laura\\anaconda3\\lib\\site-packages\\torch_geometric\\loader\\dataloader.py\", line 20, in __call__\n",
      "    return Batch.from_data_list(batch, self.follow_batch,\n",
      "  File \"c:\\Users\\laura\\anaconda3\\lib\\site-packages\\torch_geometric\\data\\batch.py\", line 76, in from_data_list\n",
      "    batch, slice_dict, inc_dict = collate(\n",
      "  File \"c:\\Users\\laura\\anaconda3\\lib\\site-packages\\torch_geometric\\data\\collate.py\", line 46, in collate\n",
      "    for store in data.stores:\n",
      "  File \"c:\\Users\\laura\\anaconda3\\lib\\site-packages\\torch_geometric\\data\\data.py\", line 499, in stores\n",
      "    return [self._store]\n",
      "  File \"c:\\Users\\laura\\anaconda3\\lib\\site-packages\\torch_geometric\\data\\data.py\", line 436, in __getattr__\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The 'data' object was created by an older version of PyG. If this error occurred while loading an already existing dataset, remove the 'processed/' directory in the dataset's root folder and try again.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\laura\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"c:\\Users\\laura\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"c:\\Users\\laura\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"c:\\Users\\laura\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"c:\\Users\\laura\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 953, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"c:\\Users\\laura\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1005, in get_records\n",
      "    style = stack_data.style_with_executing_node(style, self._tb_highlight)\n",
      "  File \"c:\\Users\\laura\\anaconda3\\lib\\site-packages\\stack_data\\core.py\", line 424, in style_with_executing_node\n",
      "    class NewStyle(style):\n",
      "  File \"c:\\Users\\laura\\anaconda3\\lib\\site-packages\\pygments\\style.py\", line 91, in __new__\n",
      "    ndef[4] = colorformat(styledef[3:])\n",
      "  File \"c:\\Users\\laura\\anaconda3\\lib\\site-packages\\pygments\\style.py\", line 58, in colorformat\n",
      "    assert False, \"wrong color format %r\" % text\n",
      "AssertionError: wrong color format 'ansiyellow'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from metrics import *\n",
    "\n",
    "# utilities\n",
    "def train(model, trainloader, optimizer, epoch):\n",
    "    print(f'Training on {len(trainloader)} samples.....')\n",
    "    model.train()\n",
    "    loss_func = nn.MSELoss()\n",
    "    predictions_tr = torch.Tensor()\n",
    "    scheduler = MultiStepLR(optimizer, milestones=[1, 5], gamma=0.5)\n",
    "    labels_tr = torch.Tensor()\n",
    "    for count, (prot_1, prot_2, label) in enumerate(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        # output = model(prot_1, prot_2)\n",
    "        # predictions_tr = torch.cat((predictions_tr, output.cpu()), 0)\n",
    "        # labels_tr = torch.cat((labels_tr, label.view(-1, 1).cpu()), 0)\n",
    "        # loss = loss_func(output, label.view(-1, 1).float())\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "    # scheduler.step()\n",
    "    # labels_tr = labels_tr.detach().numpy()\n",
    "    # predictions_tr = predictions_tr.detach().numpy()\n",
    "    # acc_tr = get_accuracy(labels_tr, predictions_tr, 0.5)\n",
    "    # print(f'Epoch {epoch-1} / 30 [==============================] - train_loss : {loss} - train_accuracy : {acc_tr}')\n",
    "\n",
    "\n",
    "def predict(model, loader):\n",
    "    model.eval()\n",
    "    predictions = torch.Tensor()\n",
    "    labels = torch.Tensor()\n",
    "    with torch.no_grad():\n",
    "        for prot_1, prot_2, label in loader:\n",
    "            output = model(prot_1, prot_2)\n",
    "            predictions = torch.cat((predictions, output.cpu()), 0)\n",
    "            labels = torch.cat((labels, label.view(-1, 1).cpu()), 0)\n",
    "    labels = labels.numpy()\n",
    "    predictions = predictions.numpy()\n",
    "    return labels.flatten(), predictions.flatten()\n",
    "\n",
    "\n",
    "# training\n",
    "n_epochs_stop = 6\n",
    "epochs_no_improve = 0\n",
    "early_stop = False\n",
    "\n",
    "model = GCNN()\n",
    "num_epochs = 50\n",
    "loss_func = nn.MSELoss()\n",
    "min_loss = 100\n",
    "best_accuracy = 0\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "print(optimizer)\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "for epoch in range(1):\n",
    "    train(model, trainloader, optimizer, epoch+1)\n",
    "\n",
    "#     G, P = predict(model, testloader)\n",
    "#     loss = get_mse(G, P)\n",
    "#     accuracy = get_accuracy(G, P, 0.5)\n",
    "#     print(f'Epoch {epoch} / {num_epochs} [==============================] - val_loss : {loss} - val_accuracy : {accuracy}')\n",
    "#     if accuracy > best_accuracy:\n",
    "#         best_accuracy = accuracy\n",
    "#         best_acc_epoch = epoch\n",
    "#         torch.save(model.state_dict(), \"../human_features/GCN.pth\")  # path to save the model\n",
    "#         print(\"Model\")\n",
    "#     if loss < min_loss:\n",
    "#         epochs_no_improve = 0\n",
    "#         min_loss = loss\n",
    "#         min_loss_epoch = epoch\n",
    "#     elif loss > min_loss:\n",
    "#         epochs_no_improve += 1\n",
    "#     if epoch > 5 and epochs_no_improve == n_epochs_stop:\n",
    "#         print('Early stopping!')\n",
    "#         early_stop = True\n",
    "#         break\n",
    "\n",
    "# print(f'min_val_loss : {min_loss} for epoch {min_loss_epoch} ............... best_val_accuracy : {best_accuracy} for epoch {best_acc_epoch}')\n",
    "# print(\"Model saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb14aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa8f9cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9cb534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df33ac6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb57cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4915fa43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f728396e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d828e086",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NP_663777</td>\n",
       "      <td>Q13114</td>\n",
       "      <td>1FLK</td>\n",
       "      <td>NP_001233</td>\n",
       "      <td>P26842</td>\n",
       "      <td>5TL5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NP_003630</td>\n",
       "      <td>Q9Y6K9</td>\n",
       "      <td>2JVX</td>\n",
       "      <td>NP_001073594</td>\n",
       "      <td>Q14790</td>\n",
       "      <td>1F9E</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NP_003001</td>\n",
       "      <td>P45985</td>\n",
       "      <td>3ALN</td>\n",
       "      <td>NP_006292</td>\n",
       "      <td>Q12852</td>\n",
       "      <td>5CEN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NP_060901</td>\n",
       "      <td>Q9P104</td>\n",
       "      <td>1J0W</td>\n",
       "      <td>NP_000199</td>\n",
       "      <td>P06213</td>\n",
       "      <td>1GAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NP_001020276</td>\n",
       "      <td>P48729</td>\n",
       "      <td>5FQD</td>\n",
       "      <td>NP_001895</td>\n",
       "      <td>P35222</td>\n",
       "      <td>1G3J</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22212</th>\n",
       "      <td>NP_000240</td>\n",
       "      <td>P40692</td>\n",
       "      <td>3RBN</td>\n",
       "      <td>NP_000404</td>\n",
       "      <td>P14061</td>\n",
       "      <td>1A27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22213</th>\n",
       "      <td>P10619</td>\n",
       "      <td>P10619</td>\n",
       "      <td>1IVY</td>\n",
       "      <td>NP_005989</td>\n",
       "      <td>P49368</td>\n",
       "      <td>6NR8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22214</th>\n",
       "      <td>NP_000779</td>\n",
       "      <td>P27707</td>\n",
       "      <td>1P5Z</td>\n",
       "      <td>NP_005557</td>\n",
       "      <td>P00338</td>\n",
       "      <td>1I10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22215</th>\n",
       "      <td>NP_008828</td>\n",
       "      <td>P31274</td>\n",
       "      <td>2LP0</td>\n",
       "      <td>NP_434700</td>\n",
       "      <td>Q9H257</td>\n",
       "      <td>6E25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22216</th>\n",
       "      <td>NP_001740</td>\n",
       "      <td>P04632</td>\n",
       "      <td>1KFU</td>\n",
       "      <td>NP_004831</td>\n",
       "      <td>Q15052</td>\n",
       "      <td>1UJY</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22217 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0       1     2             3       4     5  6\n",
       "0         NP_663777  Q13114  1FLK     NP_001233  P26842  5TL5  1\n",
       "1         NP_003630  Q9Y6K9  2JVX  NP_001073594  Q14790  1F9E  1\n",
       "2         NP_003001  P45985  3ALN     NP_006292  Q12852  5CEN  1\n",
       "3         NP_060901  Q9P104  1J0W     NP_000199  P06213  1GAG  1\n",
       "4      NP_001020276  P48729  5FQD     NP_001895  P35222  1G3J  1\n",
       "...             ...     ...   ...           ...     ...   ... ..\n",
       "22212     NP_000240  P40692  3RBN     NP_000404  P14061  1A27  0\n",
       "22213        P10619  P10619  1IVY     NP_005989  P49368  6NR8  0\n",
       "22214     NP_000779  P27707  1P5Z     NP_005557  P00338  1I10  0\n",
       "22215     NP_008828  P31274  2LP0     NP_434700  Q9H257  6E25  0\n",
       "22216     NP_001740  P04632  1KFU     NP_004831  Q15052  1UJY  1\n",
       "\n",
       "[22217 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming final_pairs is a NumPy array containing the data\n",
    "df = pd.DataFrame(final_pairs)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d6b179e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import train_test_split_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bfb5a4ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NP_663777</td>\n",
       "      <td>Q13114</td>\n",
       "      <td>1FLK</td>\n",
       "      <td>NP_001233</td>\n",
       "      <td>P26842</td>\n",
       "      <td>5TL5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NP_003630</td>\n",
       "      <td>Q9Y6K9</td>\n",
       "      <td>2JVX</td>\n",
       "      <td>NP_001073594</td>\n",
       "      <td>Q14790</td>\n",
       "      <td>1F9E</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NP_003001</td>\n",
       "      <td>P45985</td>\n",
       "      <td>3ALN</td>\n",
       "      <td>NP_006292</td>\n",
       "      <td>Q12852</td>\n",
       "      <td>5CEN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NP_060901</td>\n",
       "      <td>Q9P104</td>\n",
       "      <td>1J0W</td>\n",
       "      <td>NP_000199</td>\n",
       "      <td>P06213</td>\n",
       "      <td>1GAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NP_001020276</td>\n",
       "      <td>P48729</td>\n",
       "      <td>5FQD</td>\n",
       "      <td>NP_001895</td>\n",
       "      <td>P35222</td>\n",
       "      <td>1G3J</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22212</th>\n",
       "      <td>NP_000240</td>\n",
       "      <td>P40692</td>\n",
       "      <td>3RBN</td>\n",
       "      <td>NP_000404</td>\n",
       "      <td>P14061</td>\n",
       "      <td>1A27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22213</th>\n",
       "      <td>P10619</td>\n",
       "      <td>P10619</td>\n",
       "      <td>1IVY</td>\n",
       "      <td>NP_005989</td>\n",
       "      <td>P49368</td>\n",
       "      <td>6NR8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22214</th>\n",
       "      <td>NP_000779</td>\n",
       "      <td>P27707</td>\n",
       "      <td>1P5Z</td>\n",
       "      <td>NP_005557</td>\n",
       "      <td>P00338</td>\n",
       "      <td>1I10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22215</th>\n",
       "      <td>NP_008828</td>\n",
       "      <td>P31274</td>\n",
       "      <td>2LP0</td>\n",
       "      <td>NP_434700</td>\n",
       "      <td>Q9H257</td>\n",
       "      <td>6E25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22216</th>\n",
       "      <td>NP_001740</td>\n",
       "      <td>P04632</td>\n",
       "      <td>1KFU</td>\n",
       "      <td>NP_004831</td>\n",
       "      <td>Q15052</td>\n",
       "      <td>1UJY</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22217 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0       1     2             3       4     5  6\n",
       "0         NP_663777  Q13114  1FLK     NP_001233  P26842  5TL5  1\n",
       "1         NP_003630  Q9Y6K9  2JVX  NP_001073594  Q14790  1F9E  1\n",
       "2         NP_003001  P45985  3ALN     NP_006292  Q12852  5CEN  1\n",
       "3         NP_060901  Q9P104  1J0W     NP_000199  P06213  1GAG  1\n",
       "4      NP_001020276  P48729  5FQD     NP_001895  P35222  1G3J  1\n",
       "...             ...     ...   ...           ...     ...   ... ..\n",
       "22212     NP_000240  P40692  3RBN     NP_000404  P14061  1A27  0\n",
       "22213        P10619  P10619  1IVY     NP_005989  P49368  6NR8  0\n",
       "22214     NP_000779  P27707  1P5Z     NP_005557  P00338  1I10  0\n",
       "22215     NP_008828  P31274  2LP0     NP_434700  Q9H257  6E25  0\n",
       "22216     NP_001740  P04632  1KFU     NP_004831  Q15052  1UJY  1\n",
       "\n",
       "[22217 rows x 7 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3e3f7af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(22217, 0), dtype=float64)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Data Preprocessing\n",
    "\n",
    "# Convert non-numeric columns to numeric types or exclude them\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "df_numeric = df[numeric_cols]\n",
    "\n",
    "protein_ids = pd.unique(df[[2, 5]].values.flatten())  # Extract protein IDs and get unique values\n",
    "target_labels = df[6].values  # Extract interaction labels\n",
    "features = df_numeric.iloc[:, 7:].values.astype(float)  # Extract numeric features\n",
    "features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d5c92114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    2,    4,  ..., 3825, 3749,  878],\n",
       "        [   1,    3,    5,  ..., 3162, 3777,  888]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Graph Construction\n",
    "protein_dict = {protein_ids[i]: i for i in range(len(protein_ids))}  # Create a dictionary mapping protein IDs to indices\n",
    "\n",
    "edge_index = []\n",
    "for row in df[[2, 5]].itertuples(index=False):\n",
    "    src = protein_dict[row[0]]\n",
    "    tgt = protein_dict[row[1]]\n",
    "    edge_index.append((src, tgt))\n",
    "\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "be6b6851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], size=(22217, 0))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Feature Engineering\n",
    "x = torch.tensor(features, dtype=torch.float)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "41423027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Model Architecture\n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "32c62843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=7, stop=7, step=1)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns[7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "00ad3b9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'NP_663777'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m numeric_cols \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mdifference(protein_columns)\n\u001b[0;32m      5\u001b[0m \u001b[39m# Extract features from the numeric columns\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m features \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mloc[:, numeric_cols]\u001b[39m.\u001b[39;49mvalues\u001b[39m.\u001b[39;49mastype(\u001b[39mfloat\u001b[39;49m)\n\u001b[0;32m      8\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(features, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\n\u001b[0;32m      9\u001b[0m x\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'NP_663777'"
     ]
    }
   ],
   "source": [
    "# Step 5: Training\n",
    "protein_columns = [2, 5]  # Assuming column indices 2 and 5 are the protein identifier columns\n",
    "numeric_cols = df.columns.difference(protein_columns)\n",
    "\n",
    "# Extract features from the numeric columns\n",
    "features = df.loc[:, numeric_cols].values.astype(float)\n",
    "\n",
    "x = torch.tensor(features, dtype=torch.float)\n",
    "x\n",
    "\n",
    "# #######################################\n",
    "\n",
    "\n",
    "# model = GNNModel(input_dim=features.shape[1], hidden_dim=64, num_classes=1)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# data = Data(x=x, edge_index=edge_index, y=torch.tensor(target_labels, dtype=torch.float))\n",
    "# data = train_test_split_edges(data)  # Perform train-test split for link prediction\n",
    "\n",
    "# def train():\n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "#     out = model(data.x, data.train_pos_edge_index)\n",
    "#     loss = F.binary_cross_entropy_with_logits(out.view(-1), data.train_pos_edge_label)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     return loss.item()\n",
    "\n",
    "# def test():\n",
    "#     model.eval()\n",
    "#     out = model(data.x, data.test_neg_edge_index)\n",
    "#     pred = (out.view(-1) > 0).float()\n",
    "#     acc = (pred == data.test_neg_edge_label).sum().item() / data.test_neg_edge_label.size(0)\n",
    "#     return acc\n",
    "\n",
    "# for epoch in range(1, 101):\n",
    "#     loss = train()\n",
    "#     acc = test()\n",
    "#     print(f'Epoch: {epoch}, Loss: {loss:.4f}, Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b55746e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f7b931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff99f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dc1701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9420588",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
